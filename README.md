<h1 align="center">ğŸš• NYC Taxi Analytics Pipeline</h1>

<p align="center">
  End-to-End Spark Data Engineering Project on 112M+ Records
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Apache%20Spark-PySpark-orange">
  <img src="https://img.shields.io/badge/Platform-Databricks-red">
  <img src="https://img.shields.io/badge/Data-10GB-blue">
  <img src="https://img.shields.io/badge/Architecture-Bronze%20Silver%20Gold-green">
</p>

---

## ğŸ“Œ Project Overview

This project builds a production-style data engineering pipeline using **Apache Spark on Databricks**.

A **10GB NYC Yellow Taxi dataset (112M+ rows)** is processed through a structured architecture:

<p align="center"><b>Bronze â†’ Silver â†’ Gold</b></p>

The pipeline simulates real-world ingestion, cleaning, validation, aggregation, and performance optimization.

---

## ğŸ—‚ Dataset Information

| Attribute | Details |
|---|---|
| Dataset | NYC Yellow Taxi Trip Records (2018) |
| Source | Kaggle (NYC Taxi & Limousine Commission) |
| File | taxi_2018.csv |
| Size | ~10GB |
| Rows | ~112,234,626 |
| Format | CSV |

### Dataset Includes

- Pickup & dropoff timestamps  
- Passenger count  
- Trip distance  
- Fare breakdown (fare, tax, tip, tolls)  
- Payment type  

---
<h2 align="center">ğŸ“¥ Dataset & Volume Setup</h2>

<p align="center">
  <em>The dataset is not included in this repository because it is ~10GB. Follow the steps below to configure Databricks correctly.</em>
</p>

<div style="margin-bottom: 25px;">
  <h3>1ï¸âƒ£ Download Dataset</h3>
  <p>Download the <strong>NYC Yellow Taxi 2018</strong> dataset from Kaggle:</p>
  <ul>
    <li><strong>Search:</strong> NYC Yellow Taxi Trip Records 2018</li>
    <li><strong>File Name:</strong> <code>taxi_2018.csv</code></li>
  </ul>
</div>

<div style="margin-bottom: 25px;">
  <h3>2ï¸âƒ£ Create Project Folder Structure</h3>
  <p>In Databricks, navigate to <code>Data > Volumes > workspace/default/</code> and create the following directory hierarchy:</p>
  <pre style="background-color: #f6f8fa; padding: 15px; border-radius: 8px; border: 1px dotted #0366d6; color: #24292e;">
NYC_Yellow_Taxi_2018_Project/
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ taxi_raw_data/
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ clean_valid_trips/
â”‚   â””â”€â”€ anomalies/
â””â”€â”€ gold/
    â”œâ”€â”€ daily_revenue/
    â””â”€â”€ hourly_demand/</pre>
</div>

<div style="margin-bottom: 25px;">
  <h3>3ï¸âƒ£ Upload Dataset</h3>
  <p>Create a dedicated ingestion folder and upload your raw file:</p>
  <ul>
    <li><strong>Folder:</strong> <code>workspace/default/kaggle_files/</code></li>
    <li><strong>File:</strong> <code>taxi_2018.csv</code></li>
    <li><strong>Final Path:</strong> <code>/Volumes/workspace/default/kaggle_files/taxi_2018.csv</code></li>
  </ul>
</div>

<div style="margin-bottom: 25px;">
  <h3>4ï¸âƒ£ Update Paths in Code</h3>
  <p>Ensure the following paths are configured in your Python scripts:</p>
  <div style="background-color: #fdf6e3; padding: 10px; border-radius: 5px; border-left: 5px solid #b58900;">
    <code style="color: #d33682;">INPUT_PATH</code> = "/Volumes/workspace/default/kaggle_files/taxi_2018.csv"<br>
    <code style="color: #d33682;">BRONZE_OUTPUT_PATH</code> = "/Volumes/workspace/default/NYC_Yellow_Taxi_2018_Project/bronze/taxi_raw_data/"
  </div>
</div>

<div style="margin-bottom: 25px;">
  <h3>5ï¸âƒ£ Run the Pipeline</h3>
  <p>Run the scripts in the following sequential order:</p>
  <ol>
    <li><code>bronze.py</code></li>
    <li><code>silver.py</code></li>
    <li><code>gold.py</code></li>
  </ol>
</div>

<hr />

<div style="background-color: #fffbdd; border: 1px solid #d2991d; padding: 15px; border-radius: 8px;">
  <strong>âš ï¸ Important Notes:</strong>
  <ul>
    <li>Dataset contains <strong>112M+ rows</strong>.</li>
    <li>First execution may take time; ensure Spark cluster is active.</li>
    <li>Adjust volume paths if your workspace naming differs.</li>
  </ul>
</div>

## âš™ï¸ How Dataset Was Loaded in Databricks

### Step 1 â€” Upload dataset to volume


kaggle_file/
taxi_2018.csv


### Step 2 â€” Read CSV with explicit schema

<b> python </b><br> 
df = spark.read.csv(
    "/Volumes/workspace/default/kaggle_file/taxi_2018.csv",
    header=True,
    schema=taxi_dataset_schema
)
Step 3 â€” Save as Parquet (Bronze layer)
df.write.format("parquet") \
    .mode("overwrite") \
    .save("/Volumes/workspace/default/day_20_project/bronze/taxi_raw_data/")

CSV â†’ converted to Parquet for fast analytics.

<h2 align="center">ğŸ— Project Architecture</h2>
NYC_Project/

bronze/
   taxi_raw_data/

silver/
   clean_valid_trips/
   anomalies/

gold/
   daily_revenue/
   hourly_demand/
<h2 align="center">ğŸ¥‰ Bronze Layer â€” Raw Data</h2>
Purpose

Store original structured dataset

No business logic applied

Convert CSV â†’ Parquet

Validation Performed

Row count verification (112M+ rows)

Null checks

Negative fare detection

Distance validation

Timestamp consistency check

Bronze keeps data exactly as received.

<h2 align="center">ğŸ¥ˆ Silver Layer â€” Clean & Validated Data</h2>
Purpose

Apply business rules

Parse timestamps

Engineer trip duration

Identify anomalies

Separate clean vs invalid trips

Transformations

Convert string â†’ timestamp

Calculate trip_duration_minutes

Validation Rules

Duration > 0

Fare > 0

Passenger count > 0

Pickup â‰¤ Dropoff

Outputs
clean_valid_trips/
anomalies/

Simulates real-world data quality enforcement.

<h2 align="center">ğŸ¥‡ Gold Layer â€” Business KPIs</h2>

Analytics-ready datasets for dashboards and reporting.

ğŸ“Š Daily Revenue KPI

Business Questions

Revenue per day

Total trips per day

Average trip value

Aggregations

SUM(total_amount)

COUNT(*)

AVG(total_amount)

Stored at:

gold/daily_revenue/
â° Hourly Demand Intelligence

Business Questions

Peak travel hours

Revenue by hour

Average fare trends

Trip distance patterns

Aggregations

Trip count

Total revenue

Average fare

Average trip distance

Stored at:

gold/hourly_demand/
<h2 align="center">ğŸš€ Performance Optimizations Applied</h2>

Explicit schema definition

Parquet conversion

Column pruning

Repartition before groupBy

Shuffle partition tuning

Explain plan analysis

Adaptive execution awareness

<h2 align="center">ğŸ“ˆ What This Project Demonstrates</h2>

Handling large datasets (100M+ rows)

Layered data architecture

Data quality engineering

Business KPI modeling

Spark performance tuning

Partition strategy design

Production ETL mindset

<h2 align="center">ğŸ›  Technologies Used</h2>

Apache Spark (PySpark)

Databricks

Parquet

Distributed processing

Data engineering best practices

<h2 align="center">ğŸ¯ Key Learnings</h2>

Shuffle operations must be controlled

Repartition strategy impacts performance

Always measure anomalies before cleaning

Separate raw, clean, and business layers

Build analytics tables, not just transformations

<h2 align="center">ğŸ“Œ Future Improvements</h2>

Incremental processing

Partitioned writes for Gold tables

Workflow orchestration (Airflow / Databricks Jobs)

Data quality framework integration

Small file compaction

Monitoring & alerting

<h2 align="center">ğŸ‘¨â€ğŸ’» Author</h2> <p align="center"> Spark Data Engineering practice project focused on large-scale processing, pipeline architecture, and Spark optimization. </p> 
